- Градиентный спуск — это метод, который находит минимум функции (например, ошибки) путем итеративного изменения параметров модели (весов).
- Основная идея:
	- Вычислить градиент функции ошибки по отношению к параметрам.
	- Изменить параметры в направлении, противоположном градиенту (так как градиент указывает на направление наибольшего увеличения).
- TODO Обновление весов в классическом градиентном спуске выполняется по формуле (Source required)
	- $w:=w-\eta \nabla Q(w)=w-{\frac {\eta }{n}}\sum _{i=1}^{n}\nabla Q_{i}(w),$
	- где параметр ${\displaystyle w}$,минимизирующий ${\displaystyle Q(w)}$, следует оценить.
- ## Проблема с классическим градиентным спуском
	- В случае большого объема данных вычисление градиента на всей обучающей выборке может быть очень дорогим по времени и ресурсам.
		- Например, если в выборке миллионы примеров, для каждой итерации необходимо обработать все эти примеры, чтобы вычислить градиент
	- Поэтому лучше использовать [[SGD]]